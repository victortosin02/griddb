## Environment:
- Java version: 17
- Maven version: 3.*
- Hadoop version: 3.2.4(apache distribution)

## Read-Only Files:
- src/test/*
- input/*

## Data:
Each census data is a text line with following fields:
```text
ssn:  Social security number of a person, unique to each line
family_size: Family size of each person, it's integer value
occupation: Occupation of the person, it's string value
income: Annual income of a person, it's integer value
```

Example of input census data:
```text
@ssn,family_size,occupation,income
321 786 980,6,engineer,9000000
321 786 981,3,engineer,1000000
321 786 982,7,engineer,5000000
321 786 984,10,engineer,3000000
321 786 970,1,doctor,1800000
321 786 971,1,doctor,1800000
321 786 972,3,doctor,1800000
321 786 990,4,nurse,300000
321 786 991,4,nurse,300000
321 786 920,5,farmer,90000
321 786 921,3,farmer,90000
321 786 921,6,farmer,80000
321 786 930,2,business,9000000
```

Example of output census data
```text
@occupation,min_income,max_income,min_family_size,max_family_size
business,9000000,9000000,2,2
doctor,1800000,1800000,1,3
engineer,1000000,9000000,3,10
farmer,80000,90000,3,6
nurse,300000,300000,4,4
```

## Requirements:
In this challenge, you are going to write a hadoop job which calculates the min-max income and family size for each occupations. Structurally, the project is complete but certain parts are not implemented which you have to complete. For this application, it's not necessary to have hadoop installed on your local machine.

You have to complete the implementation of following functionality:

In the package `com.blog.hadoop.mapreduce`, complete the implementation of `CensusDataMapper.java` and `CensusDataReducer.java`:
 - input is a CSV file as mentioned in the data section above
 - ignore the header line starting with character `@`
 - each output line should represent a unique occupation and corresponding min-max values for income and family size
 - output needs to be a CSV file as mentioned in the data section above
 - output file must not have header line
 - output file can be a hadoop folder containing several part files

Complete the implementation of above methods so that unit tests pass while running. You can make use of unit tests to check your progress.

## Commands
- run: 
```bash
mvn clean package
```
- install: 
```bash
mvn clean install
```
- test: 
```bash
mvn clean test
```
